k8 architecture
--------------------
master and nodes

master
===========
api server --> all  cluster interactions comes to api server. validates RBAC and provide the response. if you are creating pods, api server validates and handover to scheduler.

scheduler --> it a component to decide where your pod should run. it checks for taints and tolerations, selectors like nodeaffinity, pod affinity, node readiness,etc.

control manager
replica controller -> make sures desired number of pod replicas run all the time.
node controller --> checks desired number of nodes are running
Job controller --> A Job runs a Pod (or multiple Pods) until the work is finished successfully ‚Äî then it stops.
ServiceAccount controller --> Create default ServiceAccounts for new namespaces.
EndpointSlice controller -> Populates EndpointSlice objects (to provide a link between Services and Pods).

etcd --> key value storage for k8 cluster. all k8 configs are here
üß† What is etcd in Kubernetes?

‚û°Ô∏è etcd is a key‚Äìvalue database that stores all the cluster data of Kubernetes.

üí¨ In simple words:

etcd is like Kubernetes‚Äôs brain üß† ‚Äî
it remembers everything about your cluster (nodes, pods, configs, secrets, etc.).


üß© Example to understand

Imagine Kubernetes is a school üè´:

The Principal is the kube-apiserver
The Teachers are the Controllers, Scheduler, etc.
The Students are the Pods

Now‚Ä¶ every decision, every change, every student‚Äôs detail must be recorded somewhere, right?

That record book üìñ = etcd
Without etcd, the school (cluster) forgets who is in which class, what subjects exist, etc.


‚öôÔ∏è What exactly etcd stores?

etcd stores all the cluster state, for example:

| Data Type              | Example                              |
| ---------------------- | ------------------------------------ |
| Nodes                  | Which worker nodes exist             |
| Pods                   | Which Pods are running, their status |
| ConfigMaps             | Configuration data                   |
| Secrets                | Passwords, tokens                    |
| Deployments / Services | App definitions                      |
| Cluster info           | Roles, namespaces, networking info   |


So if your Kubernetes cluster RESTARTS, it uses etcd to restore the last known state (what was running, where, etc.).


üèóÔ∏è Where does etcd run?

etcd runs inside the Control Plane (Master Node)
Usually as a Pod called etcd
The kube-apiserver communicates with etcd constantly.

üß≠ Every time you use a command like:

kubectl get pods


Here‚Äôs what happens:


kubectl ‚Üí talks to the kube-apiserver
apiserver ‚Üí reads data from etcd
The result (pod list) comes back to you.


| Concept    | Terraform                         | Kubernetes                      |
| ---------- | --------------------------------- | ------------------------------- |
| Storage    | `.tfstate` file (local or remote) | `etcd` database                 |
| Managed by | Terraform tool                    | kube-apiserver                  |
| Type       | File-based JSON                   | Distributed key-value store     |
| Usage      | Tracks cloud infra state          | Tracks cluster & workload state |
| Backup     | Copy `.tfstate`                   | `etcdctl snapshot save`         |

So yes ‚Äî conceptually same idea (both store state),
but technically different systems (file vs. database).

| Component                   | Function                                                                    |
| --------------------------- | --------------------------------------------------------------------------- |
| **etcd**                    | Database ‚Üí stores all cluster state                                         |
| **kube-apiserver**          | Gateway ‚Üí talks to etcd and other control plane parts                       |
| **kube-controller-manager** | Ensures cluster state matches desired state (e.g., creates pods if missing) |
| **kube-scheduler**          | Decides on which node each pod should run                                   |
| **kubelet (on each node)**  | Runs pods and reports back to API server                                    |

üëâ All these components are part of the control plane,
and they all depend on etcd to store and read data.


üìä Flow example

You create a Deployment ‚Üí API Server receives it.
API Server saves it to etcd.
Controller Manager reads from etcd ‚Üí creates ReplicaSets, Pods, etc.
Scheduler picks which node ‚Üí updates etcd.
Kubelet runs Pod ‚Üí updates status in etcd.
Everything ‚Üí saved and synced in etcd ‚úÖ

| Component                   | Simple meaning    | Job                                                       |
| --------------------------- | ----------------- | --------------------------------------------------------- |
| **etcd**                    | Memory / Database | Stores everything about cluster (Pods, Nodes, Secrets...) |
| **kube-apiserver**          | Receptionist      | First contact point ‚Äî all communication goes through it   |
| **kube-controller-manager** | Manager           | Watches etcd, fixes things automatically                  |
| **kube-scheduler**          | Planner           | Decides which node runs each Pod                          |


All these 4 together = Control Plane. ‚úÖ

üß† Step 2: What is the ‚ÄúController Manager‚Äù?

The Controller Manager is just one part inside the control plane.
Its job: make sure the cluster‚Äôs actual state = desired state.

Example üëá
You said ‚ÄúI want 3 Pods of Nginx‚Äù (in Deployment YAML).

etcd stores this info.

Controller Manager checks etcd:

‚ÄúHmm, only 2 Pods are running, but 3 are needed.‚Äù

It creates 1 more Pod to fix it.

üí¨ So the Controller Manager reads data from etcd,
but it doesn‚Äôt store anything itself ‚Äî it just acts on data.



üß† Step 3: What is ‚Äúetcd‚Äù?

etcd = Database / Memory for the control plane.

It doesn‚Äôt make decisions.
It just stores everything:

Pod info
Deployment info
ConfigMaps
Secrets
Node info

Whenever anything changes in the cluster ‚Äî it‚Äôs saved to etcd.

Node components
=============
kubelet --> agent running in every node. connects the nodes and master and makes sure containers are running inside pod.
kube-proxy --> networking rules how to send traffic to pods
Container runtime --> containerd, runc, etc. runs the containers


add-ons: adds capabilities to the cluster. vpc-cni, dns, metric server, etc.

Add-ons are extra tools or features that run on top of Kubernetes to make it more powerful or easier to use.They are not built-in, but they extend Kubernetes ‚Äî just like apps you install on your phone üì±.

cart -> catalogue

cart -> catalogue:8080

http:://catalogue-preview:8080/health --> success

| Add-on                    | Purpose / What it does                                                |
| ------------------------- | --------------------------------------------------------------------- |
| **CoreDNS**               | Provides DNS (name ‚Üí IP) inside cluster (so pods can find each other) |
| **kube-proxy**            | Handles network routing inside the cluster                            |
| **Dashboard**             | Web UI for Kubernetes                                                 |
| **Metrics Server**        | Collects resource metrics (CPU, memory) for autoscaling               |
| **Ingress Controller**    | Manages external access (like load balancer)                          |
| **Network Add-ons (CNI)** | Provide pod-to-pod networking (e.g., Calico, Flannel, Weave)          |
| **Storage Add-ons (CSI)** | Manage storage volumes (e.g., EBS, Ceph, NFS)                         |
| **Monitoring tools**      | Prometheus, Grafana ‚Äî monitor your cluster                            |
| **Logging tools**         | Fluentd, Loki ‚Äî collect and view logs                                 |

‚öôÔ∏è How Add-ons work

Add-ons are just regular Kubernetes resources (Pods, Deployments, Services, etc.)

They usually run inside the kube-system namespace.

You can install them using:

kubectl apply -f <addon.yaml>

helm install <addon-name>

or sometimes built-in during cluster setup (like with kubeadm)